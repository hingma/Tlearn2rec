{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GNN Training for MILP Problems\n",
        "\n",
        "This notebook implements the training of a Graph Neural Network (GNN) for MILP problems. It includes:\n",
        "1. Setup and installation of required packages\n",
        "2. Data loading and preprocessing\n",
        "3. Model training with different configurations\n",
        "4. Visualization of training dynamics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive to access your data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages\n",
        "%pip install torch-geometric\n",
        "%pip install matplotlib numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Set the path to your project directory\n",
        "PROJECT_DIR = '/content/drive/MyDrive/Tlearn2rec'  # Modify this path as needed\n",
        "sys.path.append(PROJECT_DIR)\n",
        "\n",
        "# Import project modules\n",
        "from gnn_model import GCNPolicy\n",
        "import config\n",
        "from visualization import TrainingVisualizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define dataset and model classes\n",
        "class MILPDataset(Dataset):\n",
        "    def __init__(self, sample_files):\n",
        "        self.sample_files = sample_files\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = torch.load(self.sample_files[idx], weights_only=False)\n",
        "        return data\n",
        "\n",
        "class SupervisedContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        labels_matrix = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
        "        labels_matrix.fill_diagonal_(False)\n",
        "        \n",
        "        if not labels_matrix.any():\n",
        "            return torch.tensor(0.0, device=embeddings.device)\n",
        "\n",
        "        sim_matrix = torch.matmul(embeddings, embeddings.T)\n",
        "        logits_mask = torch.ones_like(sim_matrix).fill_diagonal_(0)\n",
        "        \n",
        "        exp_sim = torch.exp(sim_matrix / self.temperature)\n",
        "        log_prob = (sim_matrix / self.temperature) - torch.log((exp_sim * logits_mask).sum(1, keepdim=True))\n",
        "        \n",
        "        mean_log_prob_pos = (labels_matrix * log_prob).sum(1) / labels_matrix.sum(1).clamp(min=1)\n",
        "        loss = -mean_log_prob_pos\n",
        "        \n",
        "        has_positives = labels_matrix.sum(1) > 0\n",
        "        loss = loss[has_positives].mean()\n",
        "        \n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process(model, dataloader, criterion, optimizer=None, scaler=None, device='cpu', epoch=None, phase='train'):\n",
        "    mean_loss = 0\n",
        "    n_samples_processed = 0\n",
        "    is_train = optimizer is not None\n",
        "\n",
        "    if is_train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    for batch in dataloader:\n",
        "        c, ei, ev, v, v_labels, n_cs, n_vs = [\n",
        "            t.to(device) if isinstance(t, torch.Tensor) else t for t in batch\n",
        "        ]\n",
        "        batch_size = len(n_cs)\n",
        "\n",
        "        model_input = (c, ei, ev, v, n_cs, n_vs)\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        with autocast(device_type=\"cuda\", enabled=(scaler is not None)):\n",
        "            proj_embeddings, fg_labels = model(model_input, v_labels)\n",
        "\n",
        "            if proj_embeddings is not None and fg_labels is not None:\n",
        "                loss = criterion(proj_embeddings, fg_labels)\n",
        "            else:\n",
        "                loss = torch.tensor(0.0, device=device)\n",
        "        \n",
        "        if is_train:\n",
        "            if scaler is not None:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        mean_loss += loss.item() * batch_size\n",
        "        n_samples_processed += batch_size\n",
        "\n",
        "    if n_samples_processed > 0:\n",
        "        mean_loss /= n_samples_processed\n",
        "\n",
        "    return mean_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, scaler, device, \n",
        "                running_dir, max_epochs, early_stopping, patience, visualizer):\n",
        "    best_loss = np.inf\n",
        "    plateau_count = 0\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    for epoch in range(max_epochs + 1):\n",
        "        print(f\"Epoch {epoch}...\")\n",
        "\n",
        "        # Train\n",
        "        train_loss = process(model, train_loader, criterion, optimizer, scaler, device, epoch, 'train')\n",
        "        print(f\"Train Loss: {train_loss:0.3f}\")\n",
        "\n",
        "        # Validate\n",
        "        valid_loss = process(model, valid_loader, criterion, None, scaler, device, epoch, 'valid')\n",
        "        print(f\"Valid Loss: {valid_loss:0.3f}\")\n",
        "\n",
        "        # Update visualization\n",
        "        visualizer.update(epoch, train_loss, valid_loss, current_lr)\n",
        "        \n",
        "        if valid_loss < best_loss:\n",
        "            plateau_count = 0\n",
        "            best_loss = valid_loss\n",
        "            model.save_state(running_dir / 'best_params.pkl')\n",
        "            print(\"Best model so far\")\n",
        "        else:\n",
        "            plateau_count += 1\n",
        "            if plateau_count >= early_stopping:\n",
        "                print(f\"{plateau_count} epochs without improvement, early stopping\")\n",
        "                break\n",
        "            if plateau_count % patience == 0:\n",
        "                current_lr *= 0.2\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = current_lr\n",
        "                print(f\"Decreasing learning rate to {current_lr:.1e}\")\n",
        "\n",
        "        # Plot progress\n",
        "        if epoch % 10 == 0:\n",
        "            visualizer.plot_training_curves()\n",
        "            plt.show()\n",
        "            visualizer.plot_learning_rate()\n",
        "            plt.show()\n",
        "            visualizer.save_history()\n",
        "\n",
        "    return best_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration\n",
        "\n",
        "Set up the training parameters and experiment configuration below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "problem = 'facilities'  # or 'osif'\n",
        "experiment_name = 'experiment_1'\n",
        "train_size = 1.0  # fraction of training data to use\n",
        "max_epochs = 1000\n",
        "\n",
        "# Load parameters from config\n",
        "train_params = config.TRAIN_PARAMS.copy()\n",
        "model_params = config.MODEL_PARAMS\n",
        "train_params['max_epochs'] = max_epochs\n",
        "\n",
        "# Setup directories\n",
        "running_dir = Path(PROJECT_DIR) / 'models' / problem / 'GCNPolicy' / experiment_name\n",
        "os.makedirs(running_dir, exist_ok=True)\n",
        "\n",
        "# Initialize visualizer\n",
        "visualizer = TrainingVisualizer(running_dir)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize GradScaler for mixed precision training\n",
        "scaler = GradScaler(device='cuda') if device.type == 'cuda' else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "train_files = list((Path(PROJECT_DIR) / 'data/processed' / problem / 'train').glob('*.pt'))\n",
        "valid_files = list((Path(PROJECT_DIR) / 'data/processed' / problem / 'valid').glob('*.pt'))\n",
        "\n",
        "train_files = [str(x) for x in train_files]\n",
        "valid_files = [str(x) for x in valid_files]\n",
        "\n",
        "if train_size < 1.0:\n",
        "    n_train = int(len(train_files) * train_size)\n",
        "    train_files = train_files[:n_train]\n",
        "    print(f\"Using {n_train} training files\")\n",
        "\n",
        "train_dataset = MILPDataset(train_files)\n",
        "valid_dataset = MILPDataset(valid_files)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_params['batch_size'], \n",
        "                         shuffle=True, collate_fn=collate_fn, \n",
        "                         num_workers=train_params['num_workers'], \n",
        "                         pin_memory=True)\n",
        "\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=train_params['valid_batch_size'], \n",
        "                         shuffle=False, collate_fn=collate_fn, \n",
        "                         num_workers=train_params['num_workers'], \n",
        "                         pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model and training components\n",
        "model = GCNPolicy(emb_size=model_params['emb_size'])\n",
        "model.to(device)\n",
        "\n",
        "criterion = SupervisedContrastiveLoss(temperature=train_params['temperature']).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=train_params['lr'])\n",
        "\n",
        "# Train the model\n",
        "best_loss = train_model(model, train_loader, valid_loader, criterion, optimizer, scaler, \n",
        "                       device, running_dir, max_epochs, train_params['early_stopping'], \n",
        "                       train_params['patience'], visualizer)\n",
        "\n",
        "# Load best model and compute final validation loss\n",
        "model.restore_state(running_dir / 'best_params.pkl')\n",
        "final_valid_loss = process(model, valid_loader, criterion, None, scaler, device)\n",
        "print(f\"Best validation loss: {final_valid_loss:0.3f}\")\n",
        "\n",
        "# Final visualization\n",
        "visualizer.plot_training_curves(f\"Final Training Curves - {experiment_name}\")\n",
        "plt.show()\n",
        "visualizer.plot_learning_rate()\n",
        "plt.show()\n",
        "visualizer.save_history()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment with Different Configurations\n",
        "\n",
        "You can run multiple experiments with different configurations by modifying the parameters in the cells above. For example:\n",
        "\n",
        "1. Try different training data sizes:\n",
        "```python\n",
        "train_size = 0.5  # Use 50% of training data\n",
        "```\n",
        "\n",
        "2. Try different numbers of epochs:\n",
        "```python\n",
        "max_epochs = 500  # Train for 500 epochs\n",
        "```\n",
        "\n",
        "3. Try different learning rates:\n",
        "```python\n",
        "train_params['lr'] = 0.0005  # Use a different learning rate\n",
        "```\n",
        "\n",
        "Remember to give each experiment a unique name to keep track of results:\n",
        "```python\n",
        "experiment_name = 'experiment_2_half_data'\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
