{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZEps5dLb-sC",
        "outputId": "b4d9a947-4821-4695-f753-6ebec2810472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Tlearn2rec'...\n",
            "remote: Enumerating objects: 12855, done.\u001b[K\n",
            "remote: Counting objects: 100% (225/225), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 12855 (delta 178), reused 117 (delta 83), pack-reused 12630 (from 4)\u001b[K\n",
            "Receiving objects: 100% (12855/12855), 116.33 MiB | 26.84 MiB/s, done.\n",
            "Resolving deltas: 100% (7595/7595), done.\n",
            "Updating files: 100% (12667/12667), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hingma/Tlearn2rec.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV2u2dJ8c4qr",
        "outputId": "62652924-1c07-4088-94a5-c396cc32bac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: pyomo in /usr/local/lib/python3.12/dist-packages (6.9.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.12/dist-packages (from pyomo) (3.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torch_geometric pyomo networkx matplotlib wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uheJJEJWF9fh",
        "outputId": "a23fd5a7-a299-4d27-f167-126932dc72c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd ../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhlNpDSacV_c",
        "outputId": "69dc2a26-8f5b-479a-e59f-d6738aa9f2de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Tlearn2rec/new\n"
          ]
        }
      ],
      "source": [
        "%cd Tlearn2rec/new/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCcJw_X_iDU7"
      },
      "outputs": [],
      "source": [
        "# import config\n",
        "# wandb.init(\n",
        "#     # set the wandb project where this run will be logged\n",
        "#     project=project,\n",
        "\n",
        "#     # track hyperparameters and run metadata\n",
        "#     config={\n",
        "#     \"learning_rate\": config.LR,\n",
        "#     \"architecture\": architecture,\n",
        "#     \"dataset\": dataset,\n",
        "#     \"epochs\": epochs,\n",
        "#     \"batch_size\": batch_size,\n",
        "#     \"momentum\": momentum\n",
        "#     }\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29YFQJ-3IMEZ"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fjEI0P8IO02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/tlearn311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/opt/anaconda3/envs/tlearn311/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/tlearn311/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "import config_f as config\n",
        "from datasets import build_loaders, load_karate\n",
        "from model import SimpleGCN, SimpleGAT, SimpleSAGE\n",
        "from visualize import plot_embeddings_and_clusters, plot_network_clusters, plot_training_loss, plot_validation_loss, plot_karate_score\n",
        "from cluster import GraphClustering, ClusteringEvaluator\n",
        "#\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc2wiUizJ5A7"
      },
      "source": [
        "## Wandb config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0rWkVXzIJ8kB"
      },
      "outputs": [],
      "source": [
        "project = 'MILP-GNN'\n",
        "learning_rate = config.LR\n",
        "epochs = config.MAX_EPOCHS\n",
        "# architecture ='CNN'\n",
        "# dataset = 'CIFAR-10'\n",
        "batch_size = config.BATCH_SIZE\n",
        "# momentum = 0.9\n",
        "log_freq = 1\n",
        "print_freq = 1\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define hyperparameter configurations to try\n",
        "project = \"MILP-GNN\"\n",
        "hyperparameter_configs = [\n",
        "    {'architecture': 'SimpleGCN', 'learning_rate': 0.0075, 'batch_size': 16, 'epochs': 100, 'optimizer': 'adam'},\n",
        "    {'architecture': 'SimpleGCN', 'learning_rate': 0.0125, 'batch_size': 32, 'epochs': 100, 'optimizer': 'adam'},\n",
        "    {'architecture': 'SimpleGAT', 'learning_rate': 0.0075, 'batch_size': 16, 'epochs': 100, 'optimizer': 'adam'},\n",
        "    {'architecture': 'SimpleGAT', 'learning_rate': 0.0125, 'batch_size': 32, 'epochs': 100, 'optimizer': 'adam'},\n",
        "    {'architecture': 'SimpleSAGE', 'learning_rate': 0.0075, 'batch_size': 16, 'epochs': 100, 'optimizer': 'adam'},\n",
        "    {'architecture': 'SimpleSAGE', 'learning_rate': 0.0125, 'batch_size': 32, 'epochs': 100, 'optimizer': 'adam'},\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwJORapRKV1R"
      },
      "source": [
        "## initialize the run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsEx21NOKZVs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARc5TZh-IZHU"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "olP6TgB7IcK0"
      },
      "outputs": [],
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, embeddings: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        row, col = edge_index\n",
        "        pos = F.cosine_similarity(embeddings[row], embeddings[col], dim=1)\n",
        "        pos = torch.exp(pos / self.temperature)\n",
        "\n",
        "        num_nodes = embeddings.size(0)\n",
        "        neg_idx = torch.randint(0, num_nodes, (row.numel(),), device=embeddings.device)\n",
        "        neg = F.cosine_similarity(embeddings[col], embeddings[neg_idx], dim=1)\n",
        "        neg = torch.exp(neg / self.temperature)\n",
        "\n",
        "        loss = -torch.log(pos / (pos + neg + 1e-12)).mean()\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etCCGI8xIgRa"
      },
      "source": [
        "# Train for one epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdQbcmghIlgG"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module, optimizer: Optimizer, device: torch.device) -> float:\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_graphs = 0\n",
        "\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        data = data.to(device)\n",
        "        # Debug shapes\n",
        "        if getattr(config, 'DEBUG_SHAPES', False) and not getattr(model, '_printed_train_batch', False):\n",
        "            print(f\"[train] batch x: {tuple(data.x.shape)} | edge_index: {tuple(data.edge_index.shape)}\")\n",
        "            if hasattr(data, 'batch') and data.batch is not None:\n",
        "                print(f\"[train] batch vector: {tuple(data.batch.shape)}\")\n",
        "            model._printed_train_batch = True\n",
        "        #===============================================\n",
        "        optimizer.zero_grad()\n",
        "        embeddings = model(data)\n",
        "        edge_index = to_undirected(data.edge_index)\n",
        "        loss = criterion(embeddings, edge_index)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_graphs += 1\n",
        "        # log into wandb\n",
        "        if i % log_freq == log_freq - 1:\n",
        "            wandb.log({'train_loss': loss.item()})\n",
        "\n",
        "    return total_loss / max(1, total_graphs)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loss(model: nn.Module, loader: DataLoader, criterion: nn.Module, device: torch.device) -> float:\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_graphs = 0\n",
        "    for i, data in enumerate(loader,0):\n",
        "        data = data.to(device)\n",
        "        if getattr(config, 'DEBUG_SHAPES', False) and not getattr(model, '_printed_valid_batch', False):\n",
        "            print(f\"[valid] batch x: {tuple(data.x.shape)} | edge_index: {tuple(data.edge_index.shape)}\")\n",
        "            model._printed_valid_batch = True\n",
        "        embeddings = model(data)\n",
        "        loss = criterion(embeddings, to_undirected(data.edge_index))\n",
        "        total_loss += loss.item()\n",
        "        total_graphs += 1\n",
        "        # log into wandb\n",
        "        if i % log_freq == log_freq - 1:\n",
        "            wandb.log({'valid_loss': loss.item()})\n",
        "    return total_loss / max(1, total_graphs)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_on_karate(model: nn.Module, device: torch.device) -> float:\n",
        "    data = load_karate()\n",
        "    data = data.to(device)\n",
        "    # Ensure x has the expected feature dimension for the current model\n",
        "    in_channels_expected = getattr(model, 'conv1').in_channels\n",
        "    x = data.x\n",
        "    if x is None:\n",
        "        x = torch.eye(data.num_nodes, device=device)\n",
        "    if x.size(-1) < in_channels_expected:\n",
        "        # pad with zeros\n",
        "        pad = in_channels_expected - x.size(-1)\n",
        "        x = F.pad(x, (0, pad))\n",
        "    elif x.size(-1) > in_channels_expected:\n",
        "        # project with a fixed random matrix (deterministic by seed)\n",
        "        torch.manual_seed(config.SEED)\n",
        "        proj = torch.randn(x.size(-1), in_channels_expected, device=device)\n",
        "        x = x @ proj\n",
        "    data.x = x\n",
        "    embeddings = model(data)\n",
        "    # simple proxy: intra-edge cosine similarity mean (higher is better)\n",
        "    row, col = data.edge_index\n",
        "    sim = F.cosine_similarity(embeddings[row], embeddings[col], dim=1)\n",
        "    # log into wandb\n",
        "    wandb.log({'karate_score': sim.mean().item()})\n",
        "    return sim.mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6zfCkwmI0SI"
      },
      "source": [
        "# Clustering Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tNl-ip8XI3LU"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def run_clustering_pipeline(embedding: torch.Tensor,\n",
        "                            n_clusters: int | None = None,\n",
        "                            true_labels: torch.Tensor | None = None,\n",
        "                            use_gpu: bool = False):\n",
        "    \"\"\"Mirror the notebook pipeline: compute embeddings, run 3 clustering methods, evaluate.\n",
        "\n",
        "    Returns dict with embeddings, predicted labels per method, and evaluation metrics per method.\n",
        "    \"\"\"\n",
        "    if n_clusters is None and true_labels is not None:\n",
        "        unique = torch.unique(true_labels)\n",
        "        n_clusters = int(unique.numel())\n",
        "    if n_clusters is None:\n",
        "        n_clusters = 2\n",
        "\n",
        "    clusterer = GraphClustering(n_clusters=n_clusters, use_gpu=use_gpu)\n",
        "    evaluator = ClusteringEvaluator()\n",
        "\n",
        "    results_summary = {}\n",
        "    clustering_results = {}\n",
        "\n",
        "    # K-Means\n",
        "    try:\n",
        "        kmeans_labels, _ = clusterer.kmeans_clustering(embedding)\n",
        "        kmeans_results = evaluator.evaluate_clustering(true_labels, kmeans_labels, embedding)\n",
        "        # evaluator.print_results(kmeans_results, \"K-Means\")\n",
        "        results_summary['K-Means'] = kmeans_results\n",
        "        clustering_results['kmeans'] = kmeans_labels\n",
        "    except Exception as e:\n",
        "        print(f\"[clustering] K-Means failed: {e}\")\n",
        "\n",
        "    # Spectral\n",
        "    try:\n",
        "        spectral_labels, _ = clusterer.spectral_clustering(embedding)\n",
        "        spectral_results = evaluator.evaluate_clustering(true_labels, spectral_labels, embedding)\n",
        "        # evaluator.print_results(spectral_results, \"Spectral\")\n",
        "        results_summary['Spectral'] = spectral_results\n",
        "        clustering_results['spectral'] = spectral_labels\n",
        "    except Exception as e:\n",
        "        print(f\"[clustering] Spectral failed: {e}\")\n",
        "\n",
        "    # Hierarchical\n",
        "    try:\n",
        "        hierarchical_labels, _ = clusterer.hierarchical_clustering(embedding)\n",
        "        hierarchical_results = evaluator.evaluate_clustering(true_labels, hierarchical_labels, embedding)\n",
        "        # evaluator.print_results(hierarchical_results, \"Hierarchical\")\n",
        "        results_summary['Hierarchical'] = hierarchical_results\n",
        "        clustering_results['hierarchical'] = hierarchical_labels\n",
        "    except Exception as e:\n",
        "        print(f\"[clustering] Hierarchical failed: {e}\")\n",
        "\n",
        "    return {\n",
        "        'embeddings': embedding,\n",
        "        'clustering_results': clustering_results,\n",
        "        'evaluation_results': results_summary,\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_clustering_on_loader(model: nn.Module,\n",
        "                                  loader: DataLoader,\n",
        "                                  device: torch.device) -> None:\n",
        "    \"\"\"Evaluate clustering quality per-graph in a (possibly batched) loader and print averages.\n",
        "\n",
        "    - Uses true labels `y` when available to compute ARI/NMI; always computes internal metrics.\n",
        "    - Number of clusters is inferred from true labels when available; otherwise defaults to 2.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Aggregators per method -> metric -> list of values\n",
        "    agg: dict[str, dict[str, list[float]]] = {}\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        embeddings = model(batch)\n",
        "\n",
        "        # Determine graph ids in this batch (PyG provides `batch` vector when multiple graphs)\n",
        "        if hasattr(batch, 'batch') and batch.batch is not None:\n",
        "            graph_ids = torch.unique(batch.batch).tolist()\n",
        "        else:\n",
        "            graph_ids = [None]\n",
        "\n",
        "        for gid in graph_ids:\n",
        "            if gid is None:\n",
        "                node_mask = torch.ones(embeddings.size(0), dtype=torch.bool, device=device)\n",
        "            else:\n",
        "                node_mask = (batch.batch == gid)\n",
        "\n",
        "            emb_g = embeddings[node_mask]\n",
        "            y_g = batch.y[node_mask] if (hasattr(batch, 'y') and batch.y is not None) else None\n",
        "\n",
        "            if emb_g.size(0) < 2:\n",
        "                continue\n",
        "\n",
        "            n_clusters = int(torch.unique(y_g).numel()) if y_g is not None else 2\n",
        "\n",
        "            out = run_clustering_pipeline(embedding=emb_g,  # bypass model; we already have emb_g\n",
        "                                          n_clusters=n_clusters,\n",
        "                                          true_labels=y_g,\n",
        "                                          use_gpu=device.type == 'cuda')\n",
        "\n",
        "            # Collect metrics\n",
        "            for method_name, metrics in out['evaluation_results'].items():\n",
        "                if method_name not in agg:\n",
        "                    agg[method_name] = {k: [] for k in metrics.keys()}\n",
        "                for metric, value in metrics.items():\n",
        "                    if isinstance(value, float):\n",
        "                        agg[method_name][metric].append(value)\n",
        "\n",
        "    # Print averages\n",
        "    if not agg:\n",
        "        print(\"[clustering] No metrics collected.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n=== Clustering evaluation (averaged over validation graphs) ===\")\n",
        "    for method_name, metrics in agg.items():\n",
        "        print(f\"\\n-- {method_name} --\")\n",
        "        for metric, values in metrics.items():\n",
        "            if len(values) == 0:\n",
        "                continue\n",
        "            mean_val = sum(values) / len(values)\n",
        "            print(f\"{metric}: {mean_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL_lmD9fI9bz"
      },
      "source": [
        "# Main training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader, valid_loader = build_loaders(batch_size=batch_size, valid_batch_size=batch_size)\n",
        "# Infer in_channels from first training graph\n",
        "sample = next(iter(train_loader))\n",
        "in_channels = sample.x.size(-1)\n",
        "\n",
        "if getattr(config, 'DEBUG_SHAPES', False):\n",
        "    print(f\"[setup] inferred in_channels from dataset: {in_channels}\")\n",
        "    print(f\"[setup] sample x: {tuple(sample.x.shape)}, edge_index: {tuple(sample.edge_index.shape)}\")\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run(params):\n",
        "    # Extract parameters\n",
        "    learning_rate = params.get('learning_rate', 0.01)\n",
        "    architecture = params.get('architecture', 'SimpleGCN')\n",
        "    batch_size = params.get('batch_size', 64)\n",
        "    epochs = params.get('epochs', 5)\n",
        "    momentum = params.get('momentum', 0.9)\n",
        "    optimizer_name = params.get('optimizer', 'adam')\n",
        "    # ================================\n",
        "    torch.manual_seed(config.SEED)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # ======= init wandb\n",
        "    wandb.init(project=project,\n",
        "        config=params,\n",
        "        reinit=True)  # Allow multiple runs in the same process)\n",
        "    # ======= load data ================================\n",
        "    train_loader, valid_loader = build_loaders(batch_size=batch_size, valid_batch_size=batch_size)\n",
        "    # ======= initialize model =========================\n",
        "    print(\"training{model.__class__.__name__}\")\n",
        "    if architecture == 'SimpleGCN':\n",
        "        model = SimpleGCN(in_channels=in_channels, hidden_channels=config.HIDDEN_DIM, embedding_dim=config.EMBED_DIM).to(device)\n",
        "    elif architecture == 'SimpleGAT':\n",
        "        model = SimpleGAT(in_channels=in_channels, hidden_channels=config.HIDDEN_DIM, embedding_dim=config.EMBED_DIM).to(device)\n",
        "    elif architecture == 'SimpleSAGE':\n",
        "        model = SimpleSAGE(in_channels=in_channels, hidden_channels=config.HIDDEN_DIM, embedding_dim=config.EMBED_DIM).to(device)\n",
        "    # ================================\n",
        "    optimizer = Adam(model.parameters(), lr=config.LR)\n",
        "    # ================================\n",
        "    criterion = ContrastiveLoss(temperature=config.TEMPERATURE)\n",
        "    # ================================\n",
        "    best_val = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_path = config.EXPERIMENT_DIR / f'best_{architecture}_with_{batch_size}_and_{learning_rate}.pt'\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    karate_scores = []\n",
        "    for epoch in range(1, config.MAX_EPOCHS + 1):\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss = eval_loss(model, valid_loader, criterion, device)\n",
        "        karate_score = eval_on_karate(model, device)\n",
        "        #\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        karate_scores.append(karate_score)\n",
        "        #\n",
        "        print(f\"Epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f} | karate {karate_score:.4f}\")\n",
        "        #\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save({'state_dict': model.state_dict(), 'in_channels': in_channels}, best_path)\n",
        "\n",
        "    print(f\"Best {model.__class__.__name__} model saved to: {best_path}\")\n",
        "\n",
        "    plot_training_loss(train_losses)\n",
        "    plot_validation_loss(val_losses)\n",
        "    plot_karate_score(karate_scores)\n",
        "\n",
        "    # Finish the wandb run\n",
        "    wandb.finish()\n",
        "\n",
        "    # Clustering-based evaluation on validation set using ground-truth labels when available\n",
        "    # evaluate_clustering_on_loader(model, valid_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, config in enumerate(hyperparameter_configs):\n",
        "    print(f\"Running config {i+1} of {len(hyperparameter_configs)}\")\n",
        "    run(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "ewTjQaOHJA4K",
        "outputId": "c6b81bdf-1011-43e9-91c8-d3b60e94f461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[setup] inferred in_channels from dataset: 4\n",
            "[setup] sample x: (6224, 4), edge_index: (2, 16966)\n",
            "training{model.__class__.__name__}\n",
            "[train] batch x: (4929, 4) | edge_index: (2, 13345)\n",
            "[train] batch vector: (4929,)\n",
            "[SimpleGCN] conv1: in=4, out=64\n",
            "[SimpleGCN] input x: (4929, 4), edge_index: (2, 13345)\n",
            "[SimpleGCN] conv2: in=64, out=64\n",
            "[SimpleGCN] after conv1: (4929, 64)\n",
            "[SimpleGCN] conv3: in=64, out=64\n",
            "[SimpleGCN] after conv2: (4929, 64)\n",
            "[SimpleGCN] embeddings: (4929, 64)\n"
          ]
        },
        {
          "ename": "Error",
          "evalue": "You must call wandb.init() before wandb.log()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3549979684.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mkarate_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_EPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mkarate_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_on_karate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4196722638.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# log into wandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlog_freq\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
          ]
        }
      ],
      "source": [
        "# torch.manual_seed(config.SEED)\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# train_loader, valid_loader = build_loaders()\n",
        "\n",
        "# # Infer in_channels from first training graph\n",
        "# sample = next(iter(train_loader))\n",
        "# in_channels = sample.x.size(-1)\n",
        "\n",
        "# if getattr(config, 'DEBUG_SHAPES', False):\n",
        "#     print(f\"[setup] inferred in_channels from dataset: {in_channels}\")\n",
        "#     print(f\"[setup] sample x: {tuple(sample.x.shape)}, edge_index: {tuple(sample.edge_index.shape)}\")\n",
        "\n",
        "# for model in [SimpleGCN, SimpleGAT, SimpleSAGE]:\n",
        "#     print(\"training{model.__class__.__name__}\")\n",
        "#     model = model(in_channels=in_channels, hidden_channels=config.HIDDEN_DIM, embedding_dim=config.EMBED_DIM).to(device)\n",
        "#     optimizer = Adam(model.parameters(), lr=config.LR)\n",
        "#     criterion = ContrastiveLoss(temperature=config.TEMPERATURE)\n",
        "\n",
        "#     best_val = float('inf')\n",
        "#     epochs_no_improve = 0\n",
        "#     best_path = config.EXPERIMENT_DIR / f'best_{model.__class__.__name__}.pt'\n",
        "#     train_losses = []\n",
        "#     val_losses = []\n",
        "#     karate_scores = []\n",
        "#     for epoch in range(1, config.MAX_EPOCHS + 1):\n",
        "#         train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "#         val_loss = eval_loss(model, valid_loader, criterion, device)\n",
        "#         karate_score = eval_on_karate(model, device)\n",
        "#         #\n",
        "#         train_losses.append(train_loss)\n",
        "#         val_losses.append(val_loss)\n",
        "#         karate_scores.append(karate_score)\n",
        "#         #\n",
        "#         print(f\"Epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f} | karate {karate_score:.4f}\")\n",
        "#         #\n",
        "#         if val_loss < best_val - 1e-6:\n",
        "#             best_val = val_loss\n",
        "#             epochs_no_improve = 0\n",
        "#             torch.save({'state_dict': model.state_dict(), 'in_channels': in_channels}, best_path)\n",
        "\n",
        "#     print(f\"Best {model.__class__.__name__} model saved to: {best_path}\")\n",
        "\n",
        "#     plot_training_loss(train_losses)\n",
        "#     plot_validation_loss(val_losses)\n",
        "#     plot_karate_score(karate_scores)\n",
        "\n",
        "#     # Clustering-based evaluation on validation set using ground-truth labels when available\n",
        "#     # evaluate_clustering_on_loader(model, valid_loader, device)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tlearn311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
