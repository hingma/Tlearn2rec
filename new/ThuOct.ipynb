{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZEps5dLb-sC",
        "outputId": "b4d9a947-4821-4695-f753-6ebec2810472"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/hingma/Tlearn2rec.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV2u2dJ8c4qr",
        "outputId": "62652924-1c07-4088-94a5-c396cc32bac4"
      },
      "outputs": [],
      "source": [
        "%pip install torch torch_geometric pyomo networkx matplotlib wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uheJJEJWF9fh",
        "outputId": "a23fd5a7-a299-4d27-f167-126932dc72c1"
      },
      "outputs": [],
      "source": [
        "%cd ../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhlNpDSacV_c",
        "outputId": "69dc2a26-8f5b-479a-e59f-d6738aa9f2de"
      },
      "outputs": [],
      "source": [
        "%cd Tlearn2rec/new/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29YFQJ-3IMEZ"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration Parameters\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Base paths (scoped to project root)\n",
        "PROJECT_ROOT = Path.cwd().resolve()\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "MODELS_DIR = PROJECT_ROOT / 'models'\n",
        "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
        "\n",
        "# Default dataset/problem\n",
        "PROBLEM = 'facilities'\n",
        "\n",
        "# Data splits: directories already exist under data/processed/<problem>/{train,valid,test}\n",
        "TRAIN_DIR = PROCESSED_DIR / PROBLEM / 'train'\n",
        "VALID_DIR = PROCESSED_DIR / PROBLEM / 'valid'\n",
        "TEST_DIR = PROCESSED_DIR / PROBLEM / 'test'\n",
        "\n",
        "# Training hyperparameters\n",
        "SEED = 42\n",
        "BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "LR = 1e-3\n",
        "MAX_EPOCHS = 200\n",
        "PATIENCE = 10\n",
        "EARLY_STOPPING = 20\n",
        "TEMPERATURE = 0.1\n",
        "NUM_WORKERS = min(8, os.cpu_count() or 2)\n",
        "\n",
        "# Model hyperparameters (encoder dims)\n",
        "EMBED_DIM = 64\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "# Output experiment directory\n",
        "EXPERIMENT_DIR = MODELS_DIR / PROBLEM / 'UnsupervisedGNN'\n",
        "EXPERIMENT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Debug/printing controls\n",
        "DEBUG_SHAPES = True  # print tensor and layer shapes during the first steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fjEI0P8IO02"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "from datasets import build_loaders, load_karate\n",
        "from model import SimpleGCN, SimpleGAT, SimpleSAGE\n",
        "from visualize import plot_embeddings_and_clusters, plot_network_clusters, plot_training_loss, plot_validation_loss, plot_karate_score\n",
        "from cluster import GraphClustering, ClusteringEvaluator\n",
        "#\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc2wiUizJ5A7"
      },
      "source": [
        "## Wandb config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rWkVXzIJ8kB"
      },
      "outputs": [],
      "source": [
        "project = 'MILP-GNN'\n",
        "learning_rate = LR\n",
        "epochs = MAX_EPOCHS\n",
        "# architecture ='CNN'\n",
        "# dataset = 'CIFAR-10'\n",
        "batch_size = BATCH_SIZE\n",
        "# momentum = 0.9\n",
        "log_freq = 1\n",
        "print_freq = 1\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define hyperparameter configurations to try\n",
        "project = \"MILP-GNN\"\n",
        "hyperparameter_configs = [\n",
        "    {'architecture': 'SimpleGCN', 'learning_rate': 0.0075, 'batch_size': 16, 'epochs': 100, 'optimizer': 'adam'},\n",
        "    {'architecture': 'SimpleGCN', 'learning_rate': 0.0125, 'batch_size': 32, 'epochs': 100, 'optimizer': 'adam'},\n",
        "    {'architecture': 'SimpleGAT', 'learning_rate': 0.0075, 'batch_size': 16, 'epochs': 100, 'optimizer': 'adam'},\n",
        "    {'architecture': 'SimpleGAT', 'learning_rate': 0.0125, 'batch_size': 32, 'epochs': 100, 'optimizer': 'adam'},\n",
        "    {'architecture': 'SimpleSAGE', 'learning_rate': 0.0075, 'batch_size': 16, 'epochs': 100, 'optimizer': 'adam'},\n",
        "    {'architecture': 'SimpleSAGE', 'learning_rate': 0.0125, 'batch_size': 32, 'epochs': 100, 'optimizer': 'adam'},\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwJORapRKV1R"
      },
      "source": [
        "## initialize the run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsEx21NOKZVs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARc5TZh-IZHU"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olP6TgB7IcK0"
      },
      "outputs": [],
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, embeddings: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        row, col = edge_index\n",
        "        pos = F.cosine_similarity(embeddings[row], embeddings[col], dim=1)\n",
        "        pos = torch.exp(pos / self.temperature)\n",
        "\n",
        "        num_nodes = embeddings.size(0)\n",
        "        neg_idx = torch.randint(0, num_nodes, (row.numel(),), device=embeddings.device)\n",
        "        neg = F.cosine_similarity(embeddings[col], embeddings[neg_idx], dim=1)\n",
        "        neg = torch.exp(neg / self.temperature)\n",
        "\n",
        "        loss = -torch.log(pos / (pos + neg + 1e-12)).mean()\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etCCGI8xIgRa"
      },
      "source": [
        "# Train for one epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdQbcmghIlgG"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module, optimizer: Optimizer, device: torch.device) -> float:\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_graphs = 0\n",
        "\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        data = data.to(device)\n",
        "        # Debug shapes\n",
        "        if 'DEBUG_SHAPES'== False and not getattr(model, '_printed_train_batch', False):\n",
        "            print(f\"[train] batch x: {tuple(data.x.shape)} | edge_index: {tuple(data.edge_index.shape)}\")\n",
        "            if hasattr(data, 'batch') and data.batch is not None:\n",
        "                print(f\"[train] batch vector: {tuple(data.batch.shape)}\")\n",
        "            model._printed_train_batch = True\n",
        "        #===============================================\n",
        "        optimizer.zero_grad()\n",
        "        embeddings = model(data)\n",
        "        edge_index = to_undirected(data.edge_index)\n",
        "        loss = criterion(embeddings, edge_index)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_graphs += 1\n",
        "        # log into wandb\n",
        "        if i % log_freq == log_freq - 1:\n",
        "            wandb.log({'train_loss': loss.item()})\n",
        "\n",
        "    return total_loss / max(1, total_graphs)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loss(model: nn.Module, loader: DataLoader, criterion: nn.Module, device: torch.device) -> float:\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_graphs = 0\n",
        "    for i, data in enumerate(loader,0):\n",
        "        data = data.to(device)\n",
        "        if getattr(config, 'DEBUG_SHAPES', False) and not getattr(model, '_printed_valid_batch', False):\n",
        "            print(f\"[valid] batch x: {tuple(data.x.shape)} | edge_index: {tuple(data.edge_index.shape)}\")\n",
        "            model._printed_valid_batch = True\n",
        "        embeddings = model(data)\n",
        "        loss = criterion(embeddings, to_undirected(data.edge_index))\n",
        "        total_loss += loss.item()\n",
        "        total_graphs += 1\n",
        "        # log into wandb\n",
        "        if i % log_freq == log_freq - 1:\n",
        "            wandb.log({'valid_loss': loss.item()})\n",
        "    return total_loss / max(1, total_graphs)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_on_karate(model: nn.Module, device: torch.device) -> float:\n",
        "    data = load_karate()\n",
        "    data = data.to(device)\n",
        "    # Ensure x has the expected feature dimension for the current model\n",
        "    in_channels_expected = getattr(model, 'conv1').in_channels\n",
        "    x = data.x\n",
        "    if x is None:\n",
        "        x = torch.eye(data.num_nodes, device=device)\n",
        "    if x.size(-1) < in_channels_expected:\n",
        "        # pad with zeros\n",
        "        pad = in_channels_expected - x.size(-1)\n",
        "        x = F.pad(x, (0, pad))\n",
        "    elif x.size(-1) > in_channels_expected:\n",
        "        # project with a fixed random matrix (deterministic by seed)\n",
        "        torch.manual_seed(SEED)\n",
        "        proj = torch.randn(x.size(-1), in_channels_expected, device=device)\n",
        "        x = x @ proj\n",
        "    data.x = x\n",
        "    embeddings = model(data)\n",
        "    # simple proxy: intra-edge cosine similarity mean (higher is better)\n",
        "    row, col = data.edge_index\n",
        "    sim = F.cosine_similarity(embeddings[row], embeddings[col], dim=1)\n",
        "    # log into wandb\n",
        "    wandb.log({'karate_score': sim.mean().item()})\n",
        "    return sim.mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6zfCkwmI0SI"
      },
      "source": [
        "# Clustering Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNl-ip8XI3LU"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def run_clustering_pipeline(embedding: torch.Tensor,\n",
        "                            n_clusters: int | None = None,\n",
        "                            true_labels: torch.Tensor | None = None,\n",
        "                            use_gpu: bool = False):\n",
        "    \"\"\"Mirror the notebook pipeline: compute embeddings, run 3 clustering methods, evaluate.\n",
        "\n",
        "    Returns dict with embeddings, predicted labels per method, and evaluation metrics per method.\n",
        "    \"\"\"\n",
        "    if n_clusters is None and true_labels is not None:\n",
        "        unique = torch.unique(true_labels)\n",
        "        n_clusters = int(unique.numel())\n",
        "    if n_clusters is None:\n",
        "        n_clusters = 2\n",
        "\n",
        "    clusterer = GraphClustering(n_clusters=n_clusters, use_gpu=use_gpu)\n",
        "    evaluator = ClusteringEvaluator()\n",
        "\n",
        "    results_summary = {}\n",
        "    clustering_results = {}\n",
        "\n",
        "    # K-Means\n",
        "    try:\n",
        "        kmeans_labels, _ = clusterer.kmeans_clustering(embedding)\n",
        "        kmeans_results = evaluator.evaluate_clustering(true_labels, kmeans_labels, embedding)\n",
        "        # evaluator.print_results(kmeans_results, \"K-Means\")\n",
        "        results_summary['K-Means'] = kmeans_results\n",
        "        clustering_results['kmeans'] = kmeans_labels\n",
        "    except Exception as e:\n",
        "        print(f\"[clustering] K-Means failed: {e}\")\n",
        "\n",
        "    # Spectral\n",
        "    try:\n",
        "        spectral_labels, _ = clusterer.spectral_clustering(embedding)\n",
        "        spectral_results = evaluator.evaluate_clustering(true_labels, spectral_labels, embedding)\n",
        "        # evaluator.print_results(spectral_results, \"Spectral\")\n",
        "        results_summary['Spectral'] = spectral_results\n",
        "        clustering_results['spectral'] = spectral_labels\n",
        "    except Exception as e:\n",
        "        print(f\"[clustering] Spectral failed: {e}\")\n",
        "\n",
        "    # Hierarchical\n",
        "    try:\n",
        "        hierarchical_labels, _ = clusterer.hierarchical_clustering(embedding)\n",
        "        hierarchical_results = evaluator.evaluate_clustering(true_labels, hierarchical_labels, embedding)\n",
        "        # evaluator.print_results(hierarchical_results, \"Hierarchical\")\n",
        "        results_summary['Hierarchical'] = hierarchical_results\n",
        "        clustering_results['hierarchical'] = hierarchical_labels\n",
        "    except Exception as e:\n",
        "        print(f\"[clustering] Hierarchical failed: {e}\")\n",
        "\n",
        "    return {\n",
        "        'embeddings': embedding,\n",
        "        'clustering_results': clustering_results,\n",
        "        'evaluation_results': results_summary,\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_clustering_on_loader(model: nn.Module,\n",
        "                                  loader: DataLoader,\n",
        "                                  device: torch.device) -> None:\n",
        "    \"\"\"Evaluate clustering quality per-graph in a (possibly batched) loader and print averages.\n",
        "\n",
        "    - Uses true labels `y` when available to compute ARI/NMI; always computes internal metrics.\n",
        "    - Number of clusters is inferred from true labels when available; otherwise defaults to 2.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Aggregators per method -> metric -> list of values\n",
        "    agg: dict[str, dict[str, list[float]]] = {}\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        embeddings = model(batch)\n",
        "\n",
        "        # Determine graph ids in this batch (PyG provides `batch` vector when multiple graphs)\n",
        "        if hasattr(batch, 'batch') and batch.batch is not None:\n",
        "            graph_ids = torch.unique(batch.batch).tolist()\n",
        "        else:\n",
        "            graph_ids = [None]\n",
        "\n",
        "        for gid in graph_ids:\n",
        "            if gid is None:\n",
        "                node_mask = torch.ones(embeddings.size(0), dtype=torch.bool, device=device)\n",
        "            else:\n",
        "                node_mask = (batch.batch == gid)\n",
        "\n",
        "            emb_g = embeddings[node_mask]\n",
        "            y_g = batch.y[node_mask] if (hasattr(batch, 'y') and batch.y is not None) else None\n",
        "\n",
        "            if emb_g.size(0) < 2:\n",
        "                continue\n",
        "\n",
        "            n_clusters = int(torch.unique(y_g).numel()) if y_g is not None else 2\n",
        "\n",
        "            out = run_clustering_pipeline(embedding=emb_g,  # bypass model; we already have emb_g\n",
        "                                          n_clusters=n_clusters,\n",
        "                                          true_labels=y_g,\n",
        "                                          use_gpu=device.type == 'cuda')\n",
        "\n",
        "            # Collect metrics\n",
        "            for method_name, metrics in out['evaluation_results'].items():\n",
        "                if method_name not in agg:\n",
        "                    agg[method_name] = {k: [] for k in metrics.keys()}\n",
        "                for metric, value in metrics.items():\n",
        "                    if isinstance(value, float):\n",
        "                        agg[method_name][metric].append(value)\n",
        "\n",
        "    # Print averages\n",
        "    if not agg:\n",
        "        print(\"[clustering] No metrics collected.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n=== Clustering evaluation (averaged over validation graphs) ===\")\n",
        "    for method_name, metrics in agg.items():\n",
        "        print(f\"\\n-- {method_name} --\")\n",
        "        for metric, values in metrics.items():\n",
        "            if len(values) == 0:\n",
        "                continue\n",
        "            mean_val = sum(values) / len(values)\n",
        "            print(f\"{metric}: {mean_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL_lmD9fI9bz"
      },
      "source": [
        "# Main training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader, valid_loader = build_loaders(train_batch_size=batch_size, valid_batch_size=batch_size)\n",
        "# Infer in_channels from first training graph\n",
        "sample = next(iter(train_loader))\n",
        "in_channels = sample.x.size(-1)\n",
        "\n",
        "if DEBUG_SHAPES == False:\n",
        "    print(f\"[setup] inferred in_channels from dataset: {in_channels}\")\n",
        "    print(f\"[setup] sample x: {tuple(sample.x.shape)}, edge_index: {tuple(sample.edge_index.shape)}\")\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run(params):\n",
        "    # Extract parameters\n",
        "    learning_rate = params.get('learning_rate', 0.01)\n",
        "    architecture = params.get('architecture', 'SimpleGCN')\n",
        "    batch_size = params.get('batch_size', 64)\n",
        "    epochs = params.get('epochs', 5)\n",
        "    momentum = params.get('momentum', 0.9)\n",
        "    optimizer_name = params.get('optimizer', 'adam')\n",
        "    # ================================\n",
        "    torch.manual_seed(SEED)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # ======= init wandb\n",
        "    wandb.init(project=project,\n",
        "        config=params,\n",
        "        reinit=True)  # Allow multiple runs in the same process)\n",
        "    # ======= load data ================================\n",
        "    train_loader, valid_loader = build_loaders(train_batch_size=batch_size, valid_batch_size=batch_size)\n",
        "    # ======= initialize model =========================\n",
        "    print(\"training{model.__class__.__name__}\")\n",
        "    if architecture == 'SimpleGCN':\n",
        "        model = SimpleGCN(in_channels=in_channels, hidden_channels=HIDDEN_DIM, embedding_dim=EMBED_DIM).to(device)\n",
        "    elif architecture == 'SimpleGAT':\n",
        "        model = SimpleGAT(in_channels=in_channels, hidden_channels=HIDDEN_DIM, embedding_dim=EMBED_DIM).to(device)\n",
        "    elif architecture == 'SimpleSAGE':\n",
        "        model = SimpleSAGE(in_channels=in_channels, hidden_channels=HIDDEN_DIM, embedding_dim=EMBED_DIM).to(device)\n",
        "    # ================================\n",
        "    optimizer = Adam(model.parameters(), lr=LR)\n",
        "    # ================================\n",
        "    criterion = ContrastiveLoss(temperature=TEMPERATURE)\n",
        "    # ================================\n",
        "    best_val = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_path = EXPERIMENT_DIR / f'best_{architecture}_with_{batch_size}_and_{learning_rate}.pt'\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    karate_scores = []\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss = eval_loss(model, valid_loader, criterion, device)\n",
        "        karate_score = eval_on_karate(model, device)\n",
        "        #\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        karate_scores.append(karate_score)\n",
        "        #\n",
        "        print(f\"Epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f} | karate {karate_score:.4f}\")\n",
        "        #\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save({'state_dict': model.state_dict(), 'in_channels': in_channels}, best_path)\n",
        "\n",
        "    print(f\"Best {model.__class__.__name__} model saved to: {best_path}\")\n",
        "\n",
        "    plot_training_loss(train_losses)\n",
        "    plot_validation_loss(val_losses)\n",
        "    plot_karate_score(karate_scores)\n",
        "\n",
        "    # Finish the wandb run\n",
        "    wandb.finish()\n",
        "\n",
        "    # Clustering-based evaluation on validation set using ground-truth labels when available\n",
        "    # evaluate_clustering_on_loader(model, valid_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, config in enumerate(hyperparameter_configs):\n",
        "    print(f\"Running config {i+1} of {len(hyperparameter_configs)}\")\n",
        "    run(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "ewTjQaOHJA4K",
        "outputId": "c6b81bdf-1011-43e9-91c8-d3b60e94f461"
      },
      "outputs": [],
      "source": [
        "# torch.manual_seed(SEED)\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# train_loader, valid_loader = build_loaders()\n",
        "\n",
        "# # Infer in_channels from first training graph\n",
        "# sample = next(iter(train_loader))\n",
        "# in_channels = sample.x.size(-1)\n",
        "\n",
        "# if getattr(config, 'DEBUG_SHAPES', False):\n",
        "#     print(f\"[setup] inferred in_channels from dataset: {in_channels}\")\n",
        "#     print(f\"[setup] sample x: {tuple(sample.x.shape)}, edge_index: {tuple(sample.edge_index.shape)}\")\n",
        "\n",
        "# for model in [SimpleGCN, SimpleGAT, SimpleSAGE]:\n",
        "#     print(\"training{model.__class__.__name__}\")\n",
        "#     model = model(in_channels=in_channels, hidden_channels=HIDDEN_DIM, embedding_dim=EMBED_DIM).to(device)\n",
        "#     optimizer = Adam(model.parameters(), lr=LR)\n",
        "#     criterion = ContrastiveLoss(temperature=TEMPERATURE)\n",
        "\n",
        "#     best_val = float('inf')\n",
        "#     epochs_no_improve = 0\n",
        "#     best_path = EXPERIMENT_DIR / f'best_{model.__class__.__name__}.pt'\n",
        "#     train_losses = []\n",
        "#     val_losses = []\n",
        "#     karate_scores = []\n",
        "#     for epoch in range(1, MAX_EPOCHS + 1):\n",
        "#         train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "#         val_loss = eval_loss(model, valid_loader, criterion, device)\n",
        "#         karate_score = eval_on_karate(model, device)\n",
        "#         #\n",
        "#         train_losses.append(train_loss)\n",
        "#         val_losses.append(val_loss)\n",
        "#         karate_scores.append(karate_score)\n",
        "#         #\n",
        "#         print(f\"Epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f} | karate {karate_score:.4f}\")\n",
        "#         #\n",
        "#         if val_loss < best_val - 1e-6:\n",
        "#             best_val = val_loss\n",
        "#             epochs_no_improve = 0\n",
        "#             torch.save({'state_dict': model.state_dict(), 'in_channels': in_channels}, best_path)\n",
        "\n",
        "#     print(f\"Best {model.__class__.__name__} model saved to: {best_path}\")\n",
        "\n",
        "#     plot_training_loss(train_losses)\n",
        "#     plot_validation_loss(val_losses)\n",
        "#     plot_karate_score(karate_scores)\n",
        "\n",
        "#     # Clustering-based evaluation on validation set using ground-truth labels when available\n",
        "#     # evaluate_clustering_on_loader(model, valid_loader, device)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tlearn311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
